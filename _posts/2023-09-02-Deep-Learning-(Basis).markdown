---
layout: post
title: Deep Learning (Basis)
date: 2023-09-02 00:23:00 +0300
description: This is the content of the course "Dive Into Deep Learning" at Bilibili, which is taught by Mr. Mu Li. # Add post description (optional)
img: 2023-09-02-Deep-Learning-Basis/network.png # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [DL, Softmax, MLP, Regularization, Numerical Stability, BN]
comments: true
---

My Early Understanding of Basic Deep Learning Terminology

<!-- more -->

# 求导：
求导使用链式法则。下即标量对标量求导的链式法则：
\\[
y=f(u),\; u=g(x) \quad \frac{\partial y}{\partial x}=\frac{\partial y}{\partial u}\frac{\partial u}{\partial x}
\\]

<br><br>

# trick：
1. 想要加速训练，不用32位float而是16位float。
2. 验证bug存在：对于分类模型，当output为未经softmax的评估分数时：未训练时，Hinge Loss应该=C-1，C是总类别数；无论在哪一个阶段，Hinge Loss最小为0。否则你的代码应该有bug。
3. 验证bug存在：对于分类模型，当output为softmax后的概率是：未训练时，cross-entropy损失函数值应该是log(C)，C是总类别数。否则你的代码有bug。
4. 如果损失函数曲线抖动很大的话，可以：1.降低学习率；2.增大批量大小(吴恩达有讲)。
5. CNN里，卷积层数越深(即便为了计算复杂度服务会把kernel做小一点)，输出通道数越多，往往模型越精准。
6. bacth_size不宜太过大，因为重复的样本会变多，影响训练数据的多样性，模型学到的都是些重复的没用的grad，这导致收敛变慢。举个极端的例子就是：训练集的样本全是同一张图片，你选batch_size为1学得的grad等同于batch_size为10，但要最终达到同样的acc，你后者要学的轮数是前者的10倍——所以这里我们说收敛速度，单位常常是epoch。若batch_size过大导致收敛变慢，如何解决？这时候可以提高lr，或者你运气好，数据集中类别本身就多，样本多样性丰富，那batch_size大一点也没关系。但理论上来说，batch_size变大会提高训练过程中的数值稳定性，而且batch_size过小也会降低收敛速度。
7. 使用GPU要避免：频繁在CPU和GPU之间搬运数据；在GPU上有太多if-else等控制语句。
8. 对权重w做更新，据说宜用w-=lr*grad，而非w=w-lr*grad，因为后者可能会导致requires_grad==False，因为后者求得w-lr*grad得到新值存于一个tensor中，再将此tensor值存入w中，从而丧失了requires_grad原有的值。但我觉得这样也说不通啊？
9. 在train的过程中，越上层的w往往学得越快。
10. train过程中常用的一种手法是，lr随着epoch递减。这便要用到torch.optim.lr_scheduler.
11. 看懂这样的操作：net=nn.Sequential()  net.myfeature=nn.Linear(256,1000)  net.myoutput=nn.Linear(1000,1)。其中.myfeature和.myoutput相当于是在net中添加的块的名字，名字是自定义的。
12. 当模型是多变量预测时，尽量使得train set每个样本的label值足够分散(李沐课上那个案例是单样本的labels均值为0，方差较大)，这样更便于模型预测/收敛。我觉得其本质在于提高样本多样性。
13. 提高分类/回归准确率的方法是TTA，即在测试时增广，使得net对增广出来的不同数据分别输出结果，多个结果取平均。其实，你还可以不在测试时对数据做增广，你可以训练多个不同的模型，对同一个输入数据输出多种结果，结果取平均即可。
14. 学习率在训练时的变动不仅仅有cosine曲线之类的，你还可以在valid acc曲线趋于不变时把学习率往下调。
15. 其实，有时候把valid acc刷太高也没有太多意义，因为这相当于把valid dataset也拿来训练了。
16. 当一张图片很大时，若是想针对每一个像素都生成若干锚框，则计算开销太大，可以利用自编码器的思想，先用conv把图片的大小降一层，然后再生成锚框。也可以像yolo一样，不管你图片有多大，它都是按照一定的规则按h*w生成一些网格，而不是每个像素生成几个锚框，这样不论如何生成的网格/锚框都不会很多。
17. 我们要微调在imagenet上训练的resnet模型，常常对输入的图片要做一个torchvision.transforms.Normalize处理，处理的参数是rgb_mean=torch.tensor([0.485,0.456,0.406])，rgb_std=torch.tensor([0.229,0.224,0.225])，这是resnet(pretrained=True)对输入图片的像素值(0-1之间)的硬性规定，因为resnet(pretrained=True)自己在训练时就对图片的像素做了这样的处理。但我听说现在pytorch上的resnet(pretrained=True)的输入好像也不需要这样的归一化了啊，不过做一下也没错。

<br><br>

# 求导：
深度学习里面我们用得最多的是对标量的求导。因为我们求导常常是对loss变量求导，loss变量是一个标量。假如loss变量是一个向量就麻烦了，想象一下一个向量的loss对一个向量x求导，得出一个2维张量，然后再对x求导，得到一个3维张量……以此类推，张量越来越大，还怎么算呢？

人类已知的全部求导方式大概分为3种：符号求导、数值求导、自动求导。

自动求导是深度学习中常用的求导方式。

自动求导要用到一种叫计算图的东西，可以说，计算图的存在就是为了方便自动求导的。准确来说，计算图更像是有一种便于你理解自动求导的工具——因为李沐说：“学习pytorch不需要深究计算图，但tensorflow要用到”。

计算图是什么？拿求导举例子，计算图相当于链式法则的图形化体现。其实我觉得，一个算式的计算图很像存储这个算式中缀表达式的二叉树。

计算图是如何构造的？有显式的构造(tensorflow/Theano/MXNet)和隐式的构造(PyTorch/MXNet)。前者对应符号式编程；后者对应命令式编程。前者是先构造出一个符号化的数学表达式，然后最后你要计算这个算式的话，就自己再输入变量，计算结果；后者是边构造数学表达式，边告诉计算机你此时键入的未知数符号的具体数值是什么，让计算机记住，当数学表达式构造完时，算式的结果也就被计算出来了。而在pytorch深度学习的计算中，我们常常使用隐式构造计算图而非显式构造，也就是说在做一个计算时，常用命令式编程。

(下面我们考虑的都是pytorch框架里用到的知识，在讲述深度学习时，指的也是pytorch框架下做的深度学习。毕竟李沐的全部课讲的就是pytorch。)

利用计算图，自动求导也可以分为两种模式：正向累积、反向累积/反向传递。

而在深度学习中，我们用的一般是反向累积/传递。

### 自动求导的两种模式
- 链式法则：$$\frac{\partial y}{\partial x}= \frac{\partial y}{\partial u_n} \frac{\partial u_n}{\partial u_{n-1}} ... \frac{\partial u_2}{\partial u_1} \frac{\partial u_1}{\partial x}$$
- 正向累积：$$\frac{\partial y}{\partial x}= \frac{\partial y}{\partial u_n} (\frac{\partial u_n}{\partial u_{n-1}}( ... (\frac{\partial u_2}{\partial u_1} \frac{\partial u_1}{\partial x})))$$
- 反向累积、又称反向传递：$$\frac{\partial y}{\partial x}= (((\frac{\partial y}{\partial u_n} \frac{\partial u_n}{\partial u_{n-1}}) ... )\frac{\partial u_2}{\partial u_1}) \frac{\partial u_1}{\partial x}$$
  
### 反向累积/传递：
反向累积是一种自动求导模式，y(x)对x求导，且y(x)的构造方法是隐式构造。反向累积的过程是什么？代码层面上先定义好x，再由x构造起y(x)——这也就相当于构造起了y(x)的计算图，此方式乃隐式构造。于是当你构造完y(x)时，你就拿到了一张y(x)的正向计算图，且由于是隐式构造，所以计算图构造的过程也是计算图被执行的过程，计算过程的中间和最终结果也被存储在了计算图中。接下来才是反向传递的正戏：你拿到了一张正向计算函数且存储了计算的中间和最终结果的计算图，然后你就可顺着这张计算图依照链式法则反向求导了。

(上面可以补充说明的一点是：你在代码上由x步步计算得到y的过程，pytorch框架会在背后隐式帮你构造起计算图，但假如你在写某段x参与运算的代码时不想要这段代码也被pytorch用来构造计算图怎么办？你给这段代码总体加上个“with torch.no_grad()”即可。如何理解“no_grad”？grad是求导得到的梯度，no_grad就是不求导，而计算图的构造就是为求导服务的，既然你不求导了，那我pytorch就不构造计算图了。)

### 反向累积/传递总结
- 构造计算图
- 前向：执行图，存储中间结果
- 反向：从相反方向执行图
  - 去除不需要的枝
<p align="center">
  <img src="{{site.baseurl}}/assets/img/2023-09-02-Deep-Learning-Basis/求导_1.png" alt="求导_1" width="30%">
</p>
Pytorch中，反向累积的函数是backward，用法是y(x).backward()。理论上来说，y(x)可以是自定义的任何函数，故而构造出来的y(x)的计算图不一定是一棵树，有可能是一个带环的图，具体视你这个函数的正向计算图而定。

但使用y(x).backward()函数有一个前提，就是在构造y(x)之前，你要事先激活x的gard成员变量，方法是x.requires_grad_(True)，等价于x=torch.arrange(4.0, requires_grad=True)#假设你想要个x=[0., 1., 2., 3.]。注意：x.grad被激活时初始值为NoneType。

激活x的grad成员变量有多个好处：1.李沐说的，grad存储最终y(x).forward()求出来的导数值；2.我猜的，给x激活grad变量让y(x,z)调用forward方法时，知道y是从x计算过来的，forward求导是对x求导而非z。y调用forward时怎么知道自己是由x计算过来的？貌似该信息被存储在y的成员变量grad_fn中。

Grad有个奇怪的特性：当你调用一次y(x).forward()求出对x导数存储在x.grad中时(y一般为标量，x为长度为n的一维张量，则求出来的grad也是长度为n的一维张量)，若是再调用一次forward，则x.grad中的旧值不会被清除，而是会留在那，跟这一次求导出来的是做累积，产生新值。Pytorch这样设计grad的目的在于：便于存储对loss连续求导产生的累积梯度。你要是像重置x.grad也行，调用x.zero_()即可——pytorch中的方法后带有“_”一般都是用于重写调用该方法的对象的内容。

<br><br>

# 优化算法：
优化算法，优化的是一个深度学习模型的参数，得到这些参数的最优值。

我们要训练出一个模型，模型的大概样子我们已经知道，例如是最简单的线性回归模型y=wTx，但这个模型里有很多未知参数，即为w=(w1, w2, w3, ……, wn)，我们训练模型，最终要得到的是w的最优值。

一开始，我们甩给计算机的是一个带有w随机初值的y=wTx，w的随机初值肯定不是我们最终想要的最优值，我们就要用一些方法来训练y=wTx模型，使得w中的某个wi越来越接近我们想要的最优值，这些方法就叫做模型的“优化方法”。

模型的优化方法里有一种最常见的，叫做“梯度下降法GD(Gradient Descent)”，它每次对所有样本的损失函数的加和求导，并迭代优化。它就要用到我们上面讲过的求导，求导是对y=wTx模型拟合实际数据的损失函数loss求导，grad=d(loss)/d(wi)。求导的结果用于不断更新wi的值，怎么用？wi=wi” – grad * n，n在此处被称为学习率，是一个超参数。

GD太贵了，我们对其做改进，每次只从全部样本中取一个固定大小的批量来求导，这叫做“批梯度下降法BGD(Batch Gradient Descent)”。 

但是“bacth” GD算起来好贵，所以我们对它做改进，每次进来一个样本就求一次导，而不是对一批样本求导，你看一次求导变便宜了。这种叫做“随机梯度下降法SGD(Stochastic Gradient Descent)”。

但是BGD求一次导的bacth太大，而SGD求一次导只有一个样本，一轮epoch求导次数过多，所以我们二者取折衷，让BGD的每个批量的大小随着当前的求导情况而变化(怎么变化我也不知道)，得到“小批量随机梯度下降法MSGD(minibatch SGD)”。需要注意的是，若是采用小批量随机梯度下降法，则在优化模型参数的过程中可能需要逐渐减小lr的值，特别是当batch_size=1的极端情况，因为你的最终目的是得到一组模型参数使得所有样本的Loss最小，但你每次更新模型参数使用的仅仅是一个或几个样本，这可能使得你模型参数“下山的路径”并不是总朝着使得所有样本Loss最小的方向，看上去就是你的“下山路径”在震荡，特别是模型参数接近Loss最低点时，震荡最明显，总是无法收敛到最优解，你则需要降低你的lr了，以减小路径震荡，收敛到所有样本Loss最低点。
<br>

我认为，任何优化算法都有两要素：
1. 模型的样子
2. 训练所用超参数
以线性回归模型的训练举例子，线性回归模型实际上也就相当于单层神经网络，这个神经网络的参数包括权值w和阈值b。于是，对于线性回归模型来说，模型的样子可以有两种表示方法，传统的是y=xT*w+b，但我们也可以统一规范地用单层神经网络图来表示。训练所用超参数是学习率lr。

我们在实现一个实现某优化算法(例如SGD)的函数时，首先要明确这个函数的作用是不断根据损失函数对模型参数的梯度值来对模型参数进行更新。于是你要给这个函数传入的是梯度值(在pytorch中，用于w和b变量有数据成员grad，你只需要在优化算法函数外调用y.forward()，再往优化算法函数中传w和b即可)；于是这个函数需要实现知道优化算法两要素：模型的样子+训练用的超参数。

优化算法的实现还需要注意两点：
1. 若是使用的SGD等需要用到梯度的算法，则每次对模型参数做完优化后，要对所有grad作一次清零
2. 时常注意在对模型参数进行优化的过程中，不应当对已有计算图进行更新，不进行更新的方法有：
   - with torch.no_grad():#回车  
   - detach()。 detach函数貌似是tensor变量存储的模型参数w、b的方法，但具体用法我忘了。

<br><br>

# 损失函数：
不论是训练/测试/验证集，数据都是以样本为基本单位的，而每个样本中会有多个指标和一个标签label，所以一个样本的数据就是用一个行向量+一个标量(对于训练集和测试集来说才有这个标量)来表示，这个标量就是标签label，在讨论训练出的模型的语境下，我们也叫它真实值。

在实际应用中，我们关注的主要不是损失函数的形式，也就是说我们工作的重心不在于损失函数如何通过output与target的计算出损失值，而是主要关注“损失函数值”，因为我们在反向传播对模型参数求导是，不是对损失函数本身调用backward，而是对loss(output, target)计算出来的值调用backward，才能计算出模型参数的grad。

损失函数值本身也可以看做一个以模型参数为自变量的函数，它具备三要素：
1. 真实值
2. 样本指标
3. 模型参数。
也可以说：损失函数值由真实值和预测值构成。其中样本指标和模型参数共同运算出预测值，运算的法则即是模型本身。对于损失函数值而言，模型参数相当于自变量，真实值+样本指标相当于系数，故而损失函数值求导是对模型参数求导，不是对预测值求导。

理论上来说，仅仅采纳一个样本，用模型依据其指标的预测值和样本真实值的偏差也可以表示损失函数，但我们却往往不仅采纳一个样本，而是多个样本。理论上你训练模型的过程就是一轮轮不断更新模型参数的过程，每一轮更新采用的方法就是优化算法，例如SGD算法，每一轮更新你都要由损失函数对每个模型参数求导计算一组梯度，梯度的值往往用系数——即真实值+样本指标就能表示出来(eg线性回归模型)。而前面我们说过，求损失函数往往采用多个样本，你甚至可以使用训练集中的全部样本，但那样代价太高了，所以才有了“S”GD。

损失函数若是仅仅采纳一个样本，则预测值和真实值的偏差肯定是一个标量，因为真实值label本身是一个标量，而指标和模型参数运算出来的预测值也是一个标量。但若是损失函数采纳n个样本，则偏差的直接结果肯定是一个长度为n的向量，这时就要对这个向量做一定处理，例如对这个向量取范数值，得到损失函数最终的结果。

“损失函数”本身也可以表示预测值和真实值的“误差”，损失函数不同的形式则称作不同的误差，例如形如1/2*(y-y’)^2称作MSE均方误差损失函数¬——但这个公式针对的是采纳单样本的损失函数，对于一般情况下采纳多样本的损失函数，y和y’有下标i，要做加和然后取平均，“取平均”，故而叫“均方”误差损失函数。

对均方误差损失函数还有一个改进：log_rmse，先对pred和label做log，再做rmse。

我们知道，sgd每次对param做更新时要原param-lr*grad，这里的grad是损失函数对param的导数，若是损失函数对批量样本的损失值仅仅是求加和而未取平均，则sgd公式适修改为param-lr*grad/batch_size。
<br>

分类问题中常用到的损失函数：

假如分类模型中输出的结果没有经过softmax变换，则可以直接对评估分数上Hinge Loss。对于Hinge Loss中为什么不正确分类的评估分数要+1，这是为了确保正确分类的评估分数远大于不正确的，事实上，你不一定要选用1，你可以选用其他数字。
